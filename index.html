<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Direct Motion Models for Assessing Generated Videos">
  <meta name="keywords" content="TRAJAN">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Direct Motion Models for Assessing Generated Videos</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">Direct Motion Models for Assessing Generated Videos</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://k-r-allen.github.io/">Kelsey Allen</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.carldoersch.com">Carl Doersch</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://stanniszhou.github.io/">Guangyao Zhou</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://mohammedsuhail.net/">Mohammed Suhail</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://dannydriess.github.io/">Danny Driess</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=kbqjyGQAAAAJ">Ignacio Rocco</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://yuliarubanova.github.io">Yulia Rubanova</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://tkipf.github.io/">Thomas Kipf</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://msajjadi.com/">Mehdi S. M. Sajjadi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=MxxZkEcAAAAJ&hl=en">Kevin Murphy</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=IUZ-7_cAAAAJ">Joao Carreira</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.sjoerdvansteenkiste.com/">Sjoerd van Steenkiste</a><sup>2*</sup>
              </span>


            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Google DeepMind,</span>
              <span class="author-block"><sup>2</sup>Google Research,</span>
            </div>

            <div class="is-size-7">
              <span class="author-block">(*: equal contribution)</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="TODO" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/deepmind/tapnet/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="image-container-50">
            <img src="static/images/trajan_good.gif" alt="Well reconstructed video">
            <img src="static/images/trajan_bad.gif" alt="Badly reconstructed video">
            <img src="static/images/colorbar_label.png" alt="Color coding">
          </div>
            <p>
              TRAJAN is a point track autoencoder trained to reconstruct point tracks. TRAJAN enables the automated evaluation of temporal consistency in generated and corrupted videos. Reconstruction scores are calculated via the Average Jaccard (AJ) metric. Higher AJ means better point track reconstruction.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
            <p>
              A current limitation of video generative video models is that they generate plausible looking frames, but poor motion --- an issue that is not well captured by FVD and other popular methods for evaluating generated videos.
Here we go beyond FVD by developing a metric which better measures plausible object interactions and motion.
Our novel approach is based on auto-encoding point tracks and yields motion features that can be used to compare distributions of videos (as few as one generated and one ground truth, or as many as two datasets), and reconstruction errors for evaluating motion of single videos.
We show that using point tracks instead of pixel reconstruction or action recognition features results in a metric which is markedly more sensitive to temporal distortions in synthetic data, and can predict human evaluations of temporal consistency and realism in generated videos obtained from open-source models better than a wide range of alternatives.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h3 class="title is-3">Architecture</h3>
            <img src="static/images/trajan_arch.png" alt="TRAJAN Architecture">
            <p>The trajectory encoder encodes a (variable-sized) set of point trajectories into a compressed motion latent of fixed size using a Perceiver-style transformer architecture. An occlusion flag is used in the attention mask, making the representation invariant to occluded points. The decoder takes this latent and predicts for a query point the point track that goes through this point at all other times, as well as their occlusion flag. By training the autoencoder on different input and query points, the model learns to represent a dense motion field.
            </p>
        </div>
      </div>
    </div>

      <div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-full">
      <div class="content">

        <h3 class="title is-3">TRAJAN captures human evaluations of temporal consistency and realism in generated videos</h3>
        <div class="container">
          <p>
            Example videos from the EvalCrafter [3] and VideoPhy [4] datasets covering 15 different generative video models.
            TRAJAN captures human judgements of motion consistency, appearance consistency, overall realism, and object interaction realism for 100 videos sampled from each of these 15 models better than all alternatives.
          </p>
          <div id="results-carousel" class="carousel results-carousel">
            <!-- First item in the combined carousel -->
            <div class="item">
              <div class="image-container-full">
                <figure>
                <img src="supplementary/generative/evalcrafter_show_1.gif" alt="Show-1">
                <figcaption> Show-1 </figcaption>
              </figure>
              <figure>
              <img src="supplementary/generative/evalcrafter_floor33.gif" alt="Floor 33">
              <figcaption> Floor33 </figcaption>
            </figure>
            <figure>
            <img src="supplementary/generative/evalcrafter_gen2_december.gif" alt="Gen2">
            <figcaption> Gen2 </figcaption>
          </figure>
              </div>
            </div>
              <div class="item">

              <div class="image-container-full">
                <img src="supplementary/generative/CogVideoX-2B.gif" alt="CogVideoX-2B">
                <img src="supplementary/generative/CogVideoX-5B.gif" alt="CogVideoX-5B">
                <img src="supplementary/generative/evalcrafter_hotshot.gif" alt="HotShot">
              </div>
            </div>
              <div class="item">
              <div class="image-container">
                <img src="supplementary/generative/evalcrafter_modelscope.gif" alt="ModelScope">
                <img src="supplementary/generative/evalcrafter_MoonValley.gif" alt="MoonValley">
                <img src="supplementary/generative/evalcrafter_pika_v1_december_1.gif" alt="Pika">
              </div>
            </div>
            <div class="item">
            <div class="image-container">
              <img src="supplementary/generative/evalcrafter_videocrafter-v1.0.gif" alt="VideoCrafter">
              <img src="supplementary/generative/evalcrafter_zeroscope.gif" alt="ZeroScope">
              <img src="supplementary/generative/lavie.gif" alt="LaVie">
            </div>
          </div>
          <div class="item">
          <div class="image-container">
            <img src="supplementary/generative/opensora.gif" alt="OpenSora">
            <img src="supplementary/generative/svd.gif" alt="SVD">
            <img src="supplementary/generative/vc2.gif" alt="VC2">
          </div>
        </div>

            </div>

          </div>
          <figure>
            <img src="supplementary/generative/table.png" alt="Results">
            <figcaption> Spearman's rank coefficients between human ratings and automated metrics for a subset of videos from EvalCrafter and VideoPhy (higher is better). Inter-rater sigma is the standard deviation of human responses (lower is better). </figcaption>
          </figure>
</div>
</div>
</div>
</div>

  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content">

          <h3 class="title is-3">Localizing generated video errors in space and time</h3>
          <div class="container">
            <p>
The Average Jaccard (reconstruction quality -- higher AJ means higher reconstruction quality) from TRAJAN can be used to detect generated video inconsistencies at specific points in space and at specific moments in time.
Here we see that it can be used to detect the moment when the glove's appearance morphs, as well as spatially localize the glove as the problematic location in the video.
            </p>
            <div id="results-carousel" class="carousel results-carousel">
              <!-- First item in the combined carousel -->
              <div class="item">
                <div class="image-container">
                  <img src="supplementary/detailed/detailed_score_1_fullvideo.gif" alt="Full video">
                  <img src="supplementary/detailed/1_average_jaccard.png" alt="AJ over time" width=70%>
                  <img src="supplementary/detailed/1_detailed_score.gif" alt="Detailed score">
                </div>
              </div>
                <div class="item">

                <div class="image-container">
                  <img src="supplementary/detailed/detailed_score_0_fullvideo.gif" alt="Full video">
                  <img src="supplementary/detailed/0_average_jaccard.png" alt="AJ over time">
                  <img src="supplementary/detailed/0_detailed_score.gif" alt="Detailed score">
                </div>
              </div>
                <div class="item">
                <div class="image-container">
                  <img src="supplementary/detailed/detailed_score_2_fullvideo.gif" alt="Full video">
                  <img src="supplementary/detailed/2_average_jaccard.png" alt="AJ over time">
                  <img src="supplementary/detailed/2_detailed_score.gif" alt="Detailed score">
                </div>
              </div>

              </div>

            </div>
            <p>
              From left to right: full generated video, Average Jaccard (reconstruction quality) over time, Average Jaccard shown for each point track around the <b>most poorly reconstructed</b> moment in time.
Red indicates <b>worse</b> reconstruction, blue indicates <b>better</b> reconstruction.
            </p>
          </div>

        </div>
      </div>
    </div>
    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <div class="content">

            <h3 class="title is-3">Comparing motion similarities between videos with TRAJAN</h3>
            <div class="container">
              <p>
                A comparison of the motion in a generated video to its real counterpart. Videos are generated with WALT [2].
                </p>
              <figure>
                <img src="supplementary/pairwise/motion_sim.png" alt="Motion similar">
                <figurecaption>The distance between the real and generated videos of the crowd in the TRAJAN embedding space is <b>low</b> since they have <b>similar motion</b> despite differences in object appearance.
                </figurecaption>
              </figure>
              <figure>
                <img src="supplementary/pairwise/motion_diff.png" alt="Motion different">
                <figurecaption>The distance between the real and generated videos of the man in the TRAJAN embedding space is <b>high</b> since they have <b>different motion</b> despite similarities in overall appearance.
              </figurecaption>
              </figure>

              </div>
  </div>
</div>
</div>
</div>
<div class="container is-max-desktop">

            <div class="columns is-centered has-text-centered">
              <div class="column is-full">
                <h3 class="title is-3">TRAJAN also identifies synthetic spatiotemporal corruptions than several alternatives</h3>
                  <div class="image-container-full">
                    <figure>
                    <img src="supplementary/corrupted/fig3_elastic_clean.gif" alt="Full video">
                    <figcaption> Original</figcaption>
                  </figure>
                  <figure>
                    <img src="supplementary/corrupted/fig3_elastic_s.gif" alt="AJ over time">
                    <p> </p>
                    <figcaption> Spatially corrupted (S)</figcaption>
                  </figure>
                  <figure>
                    <img src="supplementary/corrupted/fig3_elastic_st.gif" alt="Detailed score">
                    <figcaption> Spatiotemporally corrupted (ST)</figcaption>
                  </figure>
                  </div>
                </div>
              </div>
            </div>
              <div class="container is-max-desktop">

                <div class="columns is-centered has-text-centered">
                  <div class="column is-full">
                <figure><img src="supplementary/corrupted/ucf101-figs.png" alt="UCF101">
                  <figcaption>
                    Comparing different methods for detecting temporal distortions on the UCF-101 dataset introduced in [1] in terms of (left) Frechet distances in latent space and (right) per-video ordinal scores. The temporal sensitivity is determined by the ratio of the distance for the Spatiotemporal (ST) corruptions divided by the Spatial (S) corruptions. TRAJAN is particularly sensitive to temporal distortions, while motion histograms and appearance-based methods perform worse.
                  </figcaption>
                </figure>
            </div>
          </div>
        </div>


  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h3 class="title is-3">Related Links</h3>
          <div class="content has-text-justified">
            <p>
              [1] <a href="https://content-debiased-fvd.github.io/">On the Content Bias in Fréchet Video Distance</a>
            <p>
              [2] <a href="https://walt-video-diffusion.github.io/">Photorealistic Video Generation with Diffusion Models</a>
            </p>
            <p>
              [3] <a href="https://evalcrafter.github.io/">EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</a>
            </p>
            <p>
              [4] <a href="https://videophy.github.io">VideoPhy: Evaluating Physical Commonsense In Video Generation</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="TODO">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/trajan-paper/trajan-paper.github.io">source code</a> of this website, which itelf is a fork of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We just ask that you link back to this page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
