<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Beyond Frechet Video Distance: a new metric for temporally consistent motion in generated video">
  <meta name="keywords" content="TRAJAN">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Beyond Frechet Video Distance: a new metric for temporally consistent motion in generated video</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">Beyond Frechet Video Distance: a new metric for temporally consistent motion in generated video</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.sjoerdvansteenkiste.com/">Sjoerd van Steenkiste</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.csail.mit.edu/danielzoran/">Daniel Zoran</a><sup>2*</sup>,
              </span>
              <span class="author-block">
                <a href="https://yangyi02.github.io">Yi Yang</a><sup>2</sup>,
              </span>              
              <span class="author-block">
                <a href="https://yuliarubanova.github.io/">Yulia Rubanova</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=NVD-BU4AAAAJ">Rishabh Kabra</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.carldoersch.com">Carl Doersch</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cnbENAEAAAAJ">Dilara Gokay</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=kbqjyGQAAAAJ">Joseph Heyward</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://e-pot.xyz/">Etienne Pot</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://qwlouse.github.io/">Klaus Greff</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://cs.stanford.edu/~dorarad/">Drew A. Hudson</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.co.uk/citations?user=GRICvzoAAAAJ">Thomas Albert Keck</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=IUZ-7_cAAAAJ">Joao Carreira</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.de/citations?user=FXNJRDoAAAAJ">Alexey Dosovitskiy</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://msajjadi.com/">Mehdi S. M. Sajjadi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://tkipf.github.io/">Thomas Kipf</a><sup>2*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Google Research,</span>
              <span class="author-block"><sup>2</sup>Google DeepMind,</span>
              <span class="author-block"><sup>3</sup>Inceptive</span>
            </div>

            <div class="is-size-7">
              <span class="author-block">(*: equal contribution)</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/pdf?id=rjSPDVdUaw"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.05927" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <img src="static/images/teaser_v1.png" alt="Description of Image">
            <p>
              MooG is a recurrent, transformer-based, video representation model that can be unrolled
              through time. MooG learns a set of “off-the-grid” latent representation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
            <p>
              Generated videos often contain motion inconsistencies, even when each individual frame looks plausible. One metric for detecting such inconsistencies is FVD, but this has been shown to be more sensitive to frame-level rather than motion-level effects, and requires access to a reference training dataset which is not always available. Here we investigate several alternatives to FVD which replace the backbone feature extraction (originally based on the I3D activity recognition model) with either motion- or appearance-based features. For each automated metric, we investigate how it can be used for evaluating both the motion consistency and realism of single videos, as well as for comparing distributions of videos. We find that an automated metric based on an autoencoder for point tracks extracted from videos best predicts human judgements of motion consistency across a large number of open-source generated video samples, but no metric predicts human realism judgements particularly well. Relative to appearance-based feature representations such as VideoMAE and I3D, the point track autoencoder feature space is also 10x more sensitive to synthetic temporal corruptions on the UCF-101 dataset. We conclude by emphasizing the separability of motion consistency and realism to human participants, and the need for metrics which can separately detect artifacts in each of these.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Video Summary</h3>
          <a href="https://recorder-v3.slideslive.com/?share=95107&s=f5e93695-de87-4b92-99a5-96eec81bf67e">
            <img src="static/images/neurips.png" alt="Video Summary">
          </a>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">

            <h3 class="title is-3">Visualizing Token Attention Maps</h3>
            <div class="container">
              <p>
                For each pixel location, at each frame, we colour code the token that has the most attention weight at that location. If the representation is stable - i.e. if the same token tracks the same content as it moves - we should see the motion of the token argmax move with the scene motion, which is the case.
              </p>
              <div id="results-carousel" class="carousel results-carousel">
                <!-- First item in the combined carousel -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/example.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/example_token_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/example_token_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/example_token_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/example_token_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
            
                <!-- Additional items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_1.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_1_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_1_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_1_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_1_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
            
                <!-- Repeat the structure for other items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_2.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_2_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_2_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_2_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_2_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
            
                <!-- Repeat the structure for other items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_3.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_3_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_3_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_3_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_3_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>

                <!-- Repeat the structure for other items -->
                <div class="item">
                  <div class="image-container">
                    <img src="supplementary/more_examples/example_4.gif" alt="Decoder Arg-Max Attention Map" style="width: 100%; height: auto;">
                  </div>
                  <div class="token-row">
                    <img src="supplementary/more_examples/example_4_1.gif" alt="Individual Token Attention Map 1" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_4_2.gif" alt="Individual Token Attention Map 2" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_4_3.gif" alt="Individual Token Attention Map 3" style="width: 25%; height: auto;">
                    <img src="supplementary/more_examples/example_4_4.gif" alt="Individual Token Attention Map 4" style="width: 25%; height: auto;">
                  </div>
                </div>
                
              </div>
              <p>
                Top Row from left to right: colour coded arg-max tokens, blended arg-max with video, ground truth video, frame predictions. Bottom row: 4 randomly selected tokens latch to specific scene elements and track them as they move
              </p>
            </div>

            <h3 class="title is-3">PCA Analysis of Tokens</h3>
            <div class="container">
              <p>
                We unroll the model over a batch of 24 short clips, each 12 frames in length. We take the predicted states of all clips across all time steps to obtain a 294912 x 512 matrix where 512 is the token size. We calculate the PCA components of this matrix and take 3 (out of 512) of the leading components and visualize them as RGB. Note the consistent cross sequence structure, relating to meaningful elements in the scene. From left to right, PCA in RGB, blended with original video, original video.
              </p>
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                  <img src="supplementary/pca/pca_1.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_2.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_3.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_4.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
                <div class="item">
                  <img src="supplementary/pca/pca_5.gif" alt="Description of Image" style="width: 100%; height: 100%;">
                </div>
              </div>
            </div>

            <h3 class="title is-3">Changing the Number of Tokens</h3>
            <div class="container">
              <p>
                Since the model has a latent set of tokens there are no parameters in the model that depends on the number of tokens. As a result we can instantiate the model with a varying number of tokens without retraining. As can be seen the model adapts elegantly, making the tokens bind to larger areas of the image but still being able to predict future frames adequatly and track scene structure. This model has been trained with 1024 tokens. Shown, from top to bottom, are 256, 512 and 1024 tokens, from right to left predictions, ground truth frames, blended argmax attention, argmax attention.
              </p>
              <div class="item">
                <img src="supplementary/test_number_of_tokens/256_tokens.gif" alt="Description of Image" style="width: 100%; height: 100%;">
              </div>
              <div class="item">
                <img src="supplementary/test_number_of_tokens/512_tokens.gif" alt="Description of Image" style="width: 100%; height: 100%;">
              </div>
              <div class="item">
                <img src="supplementary/test_number_of_tokens/1024_tokens.gif" alt="Description of Image" style="width: 100%; height: 100%;">
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h3 class="title is-3">Related Links</h3>
          <div class="content has-text-justified">
            <p>
              <a href="https://slot-attention-video.github.io/">SAVi: Conditional Object-Centric Learning from Video</a> encodes a video into a set of temporally-consistent latent variables (object slots), and is trained to predict optical flow or to reconstruct input frames.
            </p>
            <p>
              <a href="https://slot-attention-video.github.io/savi++/">SAVi++: Towards End-to-End Object-Centric Learning from Real-World Videos</a> improves SAVi by utilizing depth prediction and by adopting best practices for model scaling
              in terms of architecture design and data augmentation.
            </p>
            <p>
              <a href="https://srt-paper.github.io/">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations</a> takes few posed or unposed images of novel real-world scenes as input and produces a set-latent scene representation that is decoded to 3D videos & semantics.
            </p>            
            <p>
              <a href="https://rust-paper.github.io/">RUST: Latent Neural Scene Representations from Unposed Imagery</a> learns a latent pose space through self supervision by taking a peek at the target view during training.
            </p>
            <p>
              <a href="https://dyst-paper.github.io/">DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</a> learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2211.03726">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/moog-paper/moog-paper.github.io">source code</a> of this website, which itelf is a fork of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We just ask that you link back to this page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
